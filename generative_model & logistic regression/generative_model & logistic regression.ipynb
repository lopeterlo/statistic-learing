{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 統計學習初論 (Spring, 2019) Homework 3\n",
    "截止日期: 9AM, 2019/4/9\n",
    "請將HTML檔上傳至Ceiba作業區。回答作業時建議使用 \"三明治\" 答題法。也就是說，先說明要做什麼，然後列出程式碼與結果，最後說明這些結果的意義。作業自己做。嚴禁抄襲。不接受紙本繳交，不接受遲交。請以英文或中文作答。\n",
    "\n",
    "## 第一題 [Classification via Generative Models]\n",
    "(50%) We are going to explore the problem of identifying smartphone position through probabilistic generative models. Motion sensors in smartphones provide valuable information for researchers to understand its owners. An interesting (and more challenging) task is to identify human activities through the data recorded by motion sensors. For example, we want to know whether the smartphone owner is walking, running, or biking. In this homework problem, we are going to tackle a simpler problem. We want to know the static position of the smartphone. There are six possible positions:\n",
    "\n",
    "Phoneonback: The phone is laying on the back of the phone with the screen pointing up (away from the ground).\n",
    "Phoneonfront: The phone is laying on the back of the phone with the screen pointing towards the ground\n",
    "Phoneonbottom: The phone is standing on the bottom of the screen, meaning the bottom is pointed towards the ground\n",
    "Phoneontop: The phone is standing on the top of the screen, meaning the top is pointed towards the ground\n",
    "Phoneonleft: The phone is laying on the left side of the screen.\n",
    "Phoneonright: The phone is laying on the right side of the screen.\n",
    "The input data is the reading of the accelerometer (cf. https://en.wikipedia.org/wiki/Accelerometer) in the smartphone. We have a training dataset that contains about 28,500 data points for phones in each of the six positions. The following is some basic information of the training dataset.\n",
    "\n",
    "### Model Training\n",
    "To train probabilistic generative classifiers, we need to assume how the data are generated in one given position. Here we are going to adopt a simple assumption that the data are generated from multivariate Gaussian distributions. To train a model under this assumption, we need to compute the mean and covariance of features at each label in the training data.\n",
    "\n",
    "For each of the six labels, you should record the following information:\n",
    "\n",
    "mu: the mean vector.\n",
    "cov: the covariance matrix.\n",
    "prec: the inverse of covariance matrix.\n",
    "detcov: logarithm of the determinant of the covariance matrix.\n",
    "n: number of observations for this label.\n",
    "prior: learned prior probability of this label.\n",
    "You should store your learned parameters in a two-level dictionary structure. For example, you can access the precision of \"Phoneonback\" by amodel[\"Phoneonback\"][\"prec\"], where amodel is the learned model.\n",
    "\n",
    "### Prediction\n",
    "To predict phone positions, compute the posterior of each position given the trained model, and select the position with the largest probability. That is, given the reading  g=(gx,gy,gz)T , the probability that the phone position is k can be written as  Prob(POS=k│g)∝Prob(g│POS=k)Prob(POS=k) .\n",
    "\n",
    "The likelihood  Prob(g|POS=k)  is simply  N(g|μk,σk) , where  μk  and  σk  are learned from the training data.\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "### Q1.1: Create your mypgc class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- def fit(self, x_train, y_train): fit training data and calculate attributes of data (mean, covariance...).\n",
    "- def predict(self, fit): predict class according to input testing data.\n",
    "- def cond_pro(self, x, cls): calculate conditional probability of class cls when having input data x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mypgc() :\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train_dict = dict()\n",
    "    \n",
    "    def std(self, data):\n",
    "        ret = data.copy()\n",
    "        ret =  (ret - np.mean(ret, axis = 0))/np.std(ret, axis = 0)\n",
    "        return ret\n",
    "    \n",
    "    def fit(self, x_train, y_train):\n",
    "        x_train_std = self.std(x_train)\n",
    "        label, index = np.unique(y_train, return_inverse=True)\n",
    "        group_x = dict()\n",
    "        count = 0\n",
    "        for i in index:\n",
    "            if group_x.get(label[i]) != None:\n",
    "                group_x[label[i]].append(x_train[count])\n",
    "            else:\n",
    "                group_x[label[i]] = [x_train[count]]\n",
    "                self.train_dict[label[i]] = dict()\n",
    "            count += 1\n",
    "#         print('mu', 'cov', 'prec', 'detcov', 'n', 'prior')\n",
    "        for key, value in group_x.items():\n",
    "            self.train_dict[key]['mu'] = np.mean(value, axis = 0)\n",
    "            self.train_dict[key]['cov'] = np.cov(np.array(value), rowvar = False)\n",
    "            self.train_dict[key]['prec'] = np.linalg.inv(self.train_dict[key]['cov'])\n",
    "            self.train_dict[key]['detcov'] = np.log(np.linalg.det(self.train_dict[key]['cov']))\n",
    "            self.train_dict[key]['n'] = len(value)\n",
    "            self.train_dict[key]['prior'] = self.train_dict[key]['n'] /count\n",
    "#             print(key, self.train_dict[key]['mu'],self.train_dict[key]['cov'],self.train_dict[key]['prec'], self.train_dict[key]['detcov'],self.train_dict[key]['n'],self.train_dict[key]['prior'])\n",
    "\n",
    "    def predict(self, x):\n",
    "        max_likely = -10000\n",
    "        ret_cls = ''\n",
    "        for label in LABEL:\n",
    "            prob = self.cond_pro(x, label)\n",
    "            if prob > max_likely:\n",
    "                max_likely = prob\n",
    "                ret_cls = label\n",
    "        return ret_cls\n",
    "    \n",
    "    def cond_pro(self, x, cls):\n",
    "        u = self.train_dict[cls]['mu'] \n",
    "        u = u.reshape(len(u), 1)\n",
    "        wk = np.dot(self.train_dict[cls]['prec'], u)\n",
    "        wk0 = -0.5 * np.dot(np.dot(u.T, self.train_dict[cls]['prec']), u)[0]  + np.log(self.train_dict[cls]['prior'] )\n",
    "        ak = np.dot(wk.T, x)[0] + wk0 - 0.5 * self.train_dict[cls]['detcov']\n",
    "        numerator = ak[0]\n",
    "        return numerator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.2: Load data from \"phone_train.picke\" and train your model. List your learned model parameters in a pretty, human readable way so that the TA can easily check the correctness of your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "LABEL = ['Phoneonleft', 'Phoneonfront', 'Phoneonback', 'Phoneonbottom', 'Phoneontop', 'Phoneonright']\n",
    "\n",
    "with open('phone_train.pickle', 'rb') as fh1:\n",
    "    train_data = pickle.load(fh1)\n",
    "y_train = train_data['label'].values\n",
    "x_train = train_data.drop(['label'], axis=1).values\n",
    "pgc1 = mypgc()\n",
    "pgc1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|class|   mu (x, y, z)|     cov       |   prec  |  \n",
    "|:---:|:-----------:|:-------------:|:---------:|\n",
    "|Phoneonback| [ 0.20693328 -0.02615235  9.7963762 ] |  [ 0.00299006, -0.00195813,  0.00284766<br>[-0.00195813,  0.00229887, -0.0024254 <br> 0.00284766, -0.0024254,   0.0048876 ]| [ 943.63669582,  469.54782044, -316.78479596<br> 469.54782044, 1146.63145078,  295.42598486<br>[-316.78479596,  295.42598486,  535.76812508]  |  \n",
    "|Phoneonbottom| [0.19076794 9.78584734 0.1465572 ] | [ 0.00273883, -0.0044379,   0.00151984<br>-0.0044379,   0.01026786, -0.00328148<br>0.00151984, -0.00328148,  0.00247968]|[1229.24902343,  503.42022589,  -87.22678994<br>503.42022589,  374.93550309,  187.61601574<br> -87.22678994,  187.61601574,  705.02302834] |  \n",
    "|Phoneonfront | [ 0.11270401  0.14265129 -9.73579633] | [ 0.00173617, -0.00562249,  0.00071319<br>-0.00562249,  0.03069977, -0.00386164<br> 0.00071319, -0.00386164,  0.00185582] |   [1415.5956848,   258.4845593,    -6.15206605<br>258.4845593,    91.32078358,   90.68682325<br>  -6.15206605,   90.68682325,  729.91124637]| \n",
    "|Phoneonleft | [ 9.92639446,  0.09275717, -0.01933473]] |[ 0.00192839, -0.00619274,  0.00063551<br>-0.00619274,  0.03200844, -0.00317611<br> 0.00063551, -0.00317611,  0.00171996] |    [1369.95113577,  263.0134811,   -20.49603676<br> 263.0134811,    88.74589895,   66.6992188 <br> -20.49603676,   66.6992188 ,  712.14869845] |  \n",
    "|Phoneonright |[-9.64916017e+00,  7.86873275e-02, -7.71762134e-03] | [ 0.00076675, -0.00067075,  0.00068673<br>-0.00067075,  0.00682422, -0.00627087<br> 0.00068673, -0.00627087,  0.00773266] |    [1432.0246564,    93.75083518,  -51.14931824<br>93.75083518,  581.24293978,  463.03773646<br>-51.14931824,  463.03773646,  509.36831176] |  \n",
    "|Phoneontop | [ 8.13769555e-04 -9.69990841e+00 -1.00215800e-01]| [ 0.00184664,  0.00529325, -0.00601384<br> 0.00529325,  0.02519526, -0.02792123<br>-0.00601384, -0.02792123,  0.0331182 ] |    [1380.20106989, -186.01874711,   93.79832935<br>-186.01874711,  629.12383998,  496.62195539<br>93.79832935,  496.62195539,  465.9185559 ] |  \n",
    "\n",
    "|class|   detcov|     n       |   prior  |  \n",
    "|:---:|:-----------:|:-------------:|:------:|\n",
    "|Phoneonback| -18.987516240382195 | 28566 | 0.17095459523510295 |  \n",
    "|Phoneonbottom| -18.242307571926013|  27842   |  0.1666217825574367 |  \n",
    "|Phoneonfront | -17.331692277173115 | 29079 |    0.1740246683064328 | \n",
    "|Phoneonleft |-27.60788503403908 | 29522 |     0.17667582302494958 |  \n",
    "|Phoneonright | -18.48369965825518 | 25687  |    0.1537250818386925 |  \n",
    "|Phoneontop | -17.04131855952812 | 26401 |   0.15799804903738546 | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.3: Load the test data from \"phone_test1.pickle\" and apply your predict method. List the first 20 predictions and well as the correct labels. Compute the accuracy for your model. Accuracy is the number of correct predictions divided by total number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "with open('phone_test1.pickle', 'rb') as fh2:\n",
    "    test_data = pickle.load(fh2)\n",
    "y_test = test_data['label'].values\n",
    "x_test = test_data.drop(['label'], axis=1).values\n",
    "# print('predicted outcome', '    ','truth value')\n",
    "count = 0 \n",
    "for i in range(len(x_test)):\n",
    "    if pgc1.predict(x_test[i]) == y_test[i]:\n",
    "        count += 1\n",
    "print(count / len(x_test))\n",
    "#     print(i, '  ', pgc1.predict(x_test[i]),'         ', y_test[i])\n",
    "#     if i == 19:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    predicted outcome          truth value\n",
    "    0     Phoneonfront         Phoneonfront\n",
    "    1     Phoneonbottom        Phoneonbottom\n",
    "    2     Phoneonbottom        Phoneonbottom\n",
    "    3     Phoneonbottom        Phoneonbottom\n",
    "    4     Phoneonfront         Phoneonfront\n",
    "    5     Phoneonright         Phoneonright\n",
    "    6     Phoneonfront         Phoneonfront\n",
    "    7     Phoneontop           Phoneontop\n",
    "    8     Phoneonfront         Phoneonfront\n",
    "    9     Phoneonfront         Phoneonfront\n",
    "    10    Phoneonfront         Phoneonfront\n",
    "    11    Phoneonback          Phoneonback\n",
    "    12    Phoneonleft          Phoneonleft\n",
    "    13    Phoneonback          Phoneonback\n",
    "    14    Phoneonbottom        Phoneonbottom\n",
    "    15    Phoneonback          Phoneonback\n",
    "    16    Phoneonright         Phoneonright\n",
    "    17    Phoneonright         Phoneonright\n",
    "    18    Phoneontop           Phoneontop\n",
    "    19    Phoneonleft          Phoneonleft\n",
    "    \n",
    "The accuracy is 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二題 [Logistic Regression with L2 Regularization]\n",
    "(100%) We discussed the Logistic regression and its variation, Bayesian logistic regression that adopts L2 regularization. The Logistic regression with L2 regularization minimize the following error function (cf. file 0b2 linear model for classification.pdf):\n",
    "\n",
    "λ2wTw−∑ni=1[tnlnyn+(1−tnln(1−yn)], \n",
    "where  yn=11+exp(−wTxn)  and  tn∈{0,1}  is the label value,  xn  is the feature vector, and  w  is the regression coefficient vector.\n",
    "\n",
    "We are going to consider an extension of this model to allow different level of regularization for different regression coefficients. Consider the constant term versus other features. The coefficient of the constant term is usually not regularized in logistic regression. It is because the constant term is associated with the prior class distribution (see the discussion in generative models), and regularizing this term will force the probability of the positive class given a zero feature vector to be 0.5. This may hurt the prediction ability since the true prior class probability may indicate other more reasonable values.\n",
    "\n",
    "Another consideration is regarding the continuous-valued features and binary-valued features. We typically normalize continuous-valued features to have zero means and unit variances but keep binary-value features untouched. It makes sense to have a single regularization value for the continuous-valued features since all of them have been normalized. Similarly, if we do not have additional information, then all binary-valued features can have the same level of regularization. However, using the same regularization coefficient for the continuous-valued and binary-valued features may not be reasonable. That is, it is often beneficial to have a regularization coefficient for the continuous-valued features, and another regularization coefficient for the binary-valued features.\n",
    "\n",
    "The above discussion suggests that a more sophisticated way to regularize a logistic regression is to have three regularization coefficients: 0 for the constant,  a1  for continuous-valued features, and  a2  for the binary-valued features. It is possible to further refine the regularization coefficients. However, hyper-parameter tuning associated with more regularization coefficients may be costly.\n",
    "\n",
    "To achieve this goal, we are going to consider a variation of L2-regularized logistic regression that allow different level of regularization for each coefficient. In the following discussion, we are going to use  X  to denote the feature matrix in the training data. The i-th row in  X ,  xi , is the feature vector for the i-th training data. The last column of  X  is one unless the use does not want to include the constant term.\n",
    "\n",
    "In this model, each regression coefficient may be associated with a different regularization coefficient. Bearing with the risk of ambigulity, we (again) use the scalar  λi  to denote the regularization coefficient for  wi . The vector  w=[w1,w2,...,wD]T  is the regression coefficient vector. Let  Λ  denote the diagonal matrix that have  λi  at  Λii . Our new error function becomes:\n",
    "\n",
    "E(w)=12wTΛw−∑ni=1[tnlnyn+(1−tnln(1−yn)], \n",
    "where  yn=11+exp(−wTxn) .\n",
    "\n",
    "This model allows  wi  to have regularization coefficient  λi . If the constant term is the last element in  w , then setting  λD  to  0  allow us to unregularize the constant term. We can simply set  λi  associated with continuous-valued features to one value, and elements associated with binary-value features to another value. This will achieve our goal of a more refined regularization structure.\n",
    "\n",
    "Following the PRML text book and the class discussion, we are going to train the model using the Newton-Raphson optimization method. In order to do so, you need to derive the gradient and hession of  E(w) . Given the training dataset, we can optimize  w  via\n",
    "\n",
    "w(new)=w(old)−H−1∇E \n",
    "In order to do so, we need to have an initial vector of  w  to kick start the iteration. One way to do this is to use the closed-form solution of ridge regression:  w=(XTX+bI)−1XTt , where  t  is the vector of training lables. Simply set  b  to the average of  λi . Another way is to change the original L2 regularization term in ridge regression to  12wTΛw  and derive the new closed form solution that mathces our model.\n",
    "\n",
    "Create a Python class named mylogistic_l2 that perform model training and prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sample usage should be like the following:\n",
    "logic1 = mylogistic_l2(reg_vec = lambda_vec, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "logic1.fit(X_train, Y_train)\n",
    "ypred = logic1.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line is to create an object with the specified regularization coefficient vector, lambda_vec, and set the maximum number of iteration to 1000. The \"tol\" parameter set the stopping condition for Newton-Raphson optimization. The iteration will stop if the improvement on the error function is less than  10−5 . The \"add_intercept\" option says that we need add a column of ones to the end of X_train before model training. The length of lambda_vec, as a result should match the number of columns after adding the \"one column\" when this option is turned on.\n",
    "\n",
    "To simplify the discussion, we use 0.5 as the threshold for the positive case when making prediction.\n",
    "\n",
    "#### Implementation Restrictions\n",
    "You are allowed to use the \"building block\" libraries including numpy and scipy in your own mylogistic_l2 class. You will receive a zero score if you adopted an existing logistic regression classifier in your answer. The input features and lables for the train method should be numpy arrays. The input features and output lables for the predict method should be numpy arrays.\n",
    "\n",
    "#### Dataset\n",
    "We are going to use to \"Adult\" dataset on the UCI machine learning reposition https://archive.ics.uci.edu/ml/datasets/Adult. The goal is to predict the label values of the income column, which can be either '>50K' or '<=50K.' The dataset had splitted the training and test data, and we are going to respect this particular train-test split in model testing.\n",
    "\n",
    "To use this dataset in our model testing, you need to go through the data cleaning process so that the label value will be 1 for '>50K' and 0 otherwise. You should remove all rows with missing values. That is, if a row contain any missing values, remove that row. All features with discrete-values (for example, native-country and workclass) should be converted to \"1-of-K\" encoding. Include a particular feature value only if this unique value appears more than 10 times in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.1 (20%) Download the Adult dataset. Clean up the dataset and create x_train, y_train, x_test, y_test for training feature, training value, test feature, test label. All of these variables should be numpy arrays. Provide summary statistics for your training and test datasets so that TA can verify the correctness of your procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "COLUMN_NAMES = ['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country', 'result']\n",
    "CONTINUOUS = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "RESULT = ['result']\n",
    "DESCRETE = list(set(COLUMN_NAMES) - set(CONTINUOUS) - set(RESULT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data processed result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "train_data = pd.read_csv('adult_data.txt', sep = '\\, ', names = COLUMN_NAMES, engine = 'python')\n",
    "cleaned_train_data = data_cleaning(train_data, 'train')\n",
    "binary_train_data = pd.DataFrame()\n",
    "drop_fea = set()\n",
    "for i in DESCRETE:\n",
    "    dummy = pd.get_dummies(cleaned_train_data[i])\n",
    "    binary_train_data = pd.concat([binary_train_data, dummy], axis = 1)\n",
    "for col in binary_train_data.columns:\n",
    "    if binary_train_data[col].sum() <= 10:\n",
    "        drop_fea.add(col)\n",
    "binary_train_data = binary_train_data.drop(drop_fea, axis = 1)\n",
    "filter_train_data = cleaned_train_data[list(set(COLUMN_NAMES)- set(DESCRETE))].join(binary_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>age</th>\n",
       "      <th>education-num</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>Amer-Indian-Eskimo</th>\n",
       "      <th>Asian-Pac-Islander</th>\n",
       "      <th>Black</th>\n",
       "      <th>Other</th>\n",
       "      <th>...</th>\n",
       "      <th>Some-college</th>\n",
       "      <th>Federal-gov</th>\n",
       "      <th>Local-gov</th>\n",
       "      <th>Private</th>\n",
       "      <th>Self-emp-inc</th>\n",
       "      <th>Self-emp-not-inc</th>\n",
       "      <th>State-gov</th>\n",
       "      <th>Without-pay</th>\n",
       "      <th>Female</th>\n",
       "      <th>Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30162.000000</td>\n",
       "      <td>3.016200e+04</td>\n",
       "      <td>30162.000000</td>\n",
       "      <td>30162.000000</td>\n",
       "      <td>30162.000000</td>\n",
       "      <td>30162.000000</td>\n",
       "      <td>30162.000000</td>\n",
       "      <td>30162.000000</td>\n",
       "      <td>30162.000000</td>\n",
       "      <td>30162.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>30162.000000</td>\n",
       "      <td>30162.000000</td>\n",
       "      <td>30162.000000</td>\n",
       "      <td>30162.000000</td>\n",
       "      <td>30162.000000</td>\n",
       "      <td>30162.000000</td>\n",
       "      <td>30162.000000</td>\n",
       "      <td>30162.000000</td>\n",
       "      <td>30162.000000</td>\n",
       "      <td>30162.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1092.007858</td>\n",
       "      <td>1.897938e+05</td>\n",
       "      <td>88.372489</td>\n",
       "      <td>38.437902</td>\n",
       "      <td>10.121312</td>\n",
       "      <td>40.931238</td>\n",
       "      <td>0.009482</td>\n",
       "      <td>0.029673</td>\n",
       "      <td>0.093396</td>\n",
       "      <td>0.007659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.221404</td>\n",
       "      <td>0.031265</td>\n",
       "      <td>0.068530</td>\n",
       "      <td>0.738877</td>\n",
       "      <td>0.035608</td>\n",
       "      <td>0.082853</td>\n",
       "      <td>0.042404</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.324315</td>\n",
       "      <td>0.675685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7406.346497</td>\n",
       "      <td>1.056530e+05</td>\n",
       "      <td>404.298370</td>\n",
       "      <td>13.134665</td>\n",
       "      <td>2.549995</td>\n",
       "      <td>11.979984</td>\n",
       "      <td>0.096915</td>\n",
       "      <td>0.169687</td>\n",
       "      <td>0.290991</td>\n",
       "      <td>0.087179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.415199</td>\n",
       "      <td>0.174035</td>\n",
       "      <td>0.252657</td>\n",
       "      <td>0.439254</td>\n",
       "      <td>0.185313</td>\n",
       "      <td>0.275664</td>\n",
       "      <td>0.201513</td>\n",
       "      <td>0.021540</td>\n",
       "      <td>0.468126</td>\n",
       "      <td>0.468126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.376900e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.176272e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.784250e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.376285e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>99999.000000</td>\n",
       "      <td>1.484705e+06</td>\n",
       "      <td>4356.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       capital-gain        fnlwgt  capital-loss           age  education-num  \\\n",
       "count  30162.000000  3.016200e+04  30162.000000  30162.000000   30162.000000   \n",
       "mean    1092.007858  1.897938e+05     88.372489     38.437902      10.121312   \n",
       "std     7406.346497  1.056530e+05    404.298370     13.134665       2.549995   \n",
       "min        0.000000  1.376900e+04      0.000000     17.000000       1.000000   \n",
       "25%        0.000000  1.176272e+05      0.000000     28.000000       9.000000   \n",
       "50%        0.000000  1.784250e+05      0.000000     37.000000      10.000000   \n",
       "75%        0.000000  2.376285e+05      0.000000     47.000000      13.000000   \n",
       "max    99999.000000  1.484705e+06   4356.000000     90.000000      16.000000   \n",
       "\n",
       "       hours-per-week  Amer-Indian-Eskimo  Asian-Pac-Islander         Black  \\\n",
       "count    30162.000000        30162.000000        30162.000000  30162.000000   \n",
       "mean        40.931238            0.009482            0.029673      0.093396   \n",
       "std         11.979984            0.096915            0.169687      0.290991   \n",
       "min          1.000000            0.000000            0.000000      0.000000   \n",
       "25%         40.000000            0.000000            0.000000      0.000000   \n",
       "50%         40.000000            0.000000            0.000000      0.000000   \n",
       "75%         45.000000            0.000000            0.000000      0.000000   \n",
       "max         99.000000            1.000000            1.000000      1.000000   \n",
       "\n",
       "              Other  ...  Some-college   Federal-gov     Local-gov  \\\n",
       "count  30162.000000  ...  30162.000000  30162.000000  30162.000000   \n",
       "mean       0.007659  ...      0.221404      0.031265      0.068530   \n",
       "std        0.087179  ...      0.415199      0.174035      0.252657   \n",
       "min        0.000000  ...      0.000000      0.000000      0.000000   \n",
       "25%        0.000000  ...      0.000000      0.000000      0.000000   \n",
       "50%        0.000000  ...      0.000000      0.000000      0.000000   \n",
       "75%        0.000000  ...      0.000000      0.000000      0.000000   \n",
       "max        1.000000  ...      1.000000      1.000000      1.000000   \n",
       "\n",
       "            Private  Self-emp-inc  Self-emp-not-inc     State-gov  \\\n",
       "count  30162.000000  30162.000000      30162.000000  30162.000000   \n",
       "mean       0.738877      0.035608          0.082853      0.042404   \n",
       "std        0.439254      0.185313          0.275664      0.201513   \n",
       "min        0.000000      0.000000          0.000000      0.000000   \n",
       "25%        0.000000      0.000000          0.000000      0.000000   \n",
       "50%        1.000000      0.000000          0.000000      0.000000   \n",
       "75%        1.000000      0.000000          0.000000      0.000000   \n",
       "max        1.000000      1.000000          1.000000      1.000000   \n",
       "\n",
       "        Without-pay        Female          Male  \n",
       "count  30162.000000  30162.000000  30162.000000  \n",
       "mean       0.000464      0.324315      0.675685  \n",
       "std        0.021540      0.468126      0.468126  \n",
       "min        0.000000      0.000000      0.000000  \n",
       "25%        0.000000      0.000000      0.000000  \n",
       "50%        0.000000      0.000000      1.000000  \n",
       "75%        0.000000      1.000000      1.000000  \n",
       "max        1.000000      1.000000      1.000000  \n",
       "\n",
       "[8 rows x 102 columns]"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = filter_train_data['result'].values\n",
    "train_x = filter_train_data.drop('result',  axis=1).values\n",
    "\n",
    "filter_train_data.drop('result',  axis=1).describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Above picture shows train_x and train_y information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing data processed result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('adult_test.txt', sep = '\\, ', names = COLUMN_NAMES, engine = 'python')\n",
    "cleaned_test_data = data_cleaning(test_data, 'test')\n",
    "binary_test_data = pd.DataFrame()\n",
    "for i in DESCRETE:\n",
    "    dummy = pd.get_dummies(cleaned_test_data[i])\n",
    "    binary_test_data = pd.concat([binary_test_data, dummy], axis = 1)\n",
    "for i in drop_fea:\n",
    "    try:\n",
    "        binary_test_data = binary_test_data.drop(i, axis = 1)\n",
    "    except:\n",
    "        pass\n",
    "filter_test_data = cleaned_test_data[list(set(COLUMN_NAMES)- set(DESCRETE))].join(binary_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>age</th>\n",
       "      <th>education-num</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>Amer-Indian-Eskimo</th>\n",
       "      <th>Asian-Pac-Islander</th>\n",
       "      <th>Black</th>\n",
       "      <th>Other</th>\n",
       "      <th>...</th>\n",
       "      <th>Some-college</th>\n",
       "      <th>Federal-gov</th>\n",
       "      <th>Local-gov</th>\n",
       "      <th>Private</th>\n",
       "      <th>Self-emp-inc</th>\n",
       "      <th>Self-emp-not-inc</th>\n",
       "      <th>State-gov</th>\n",
       "      <th>Without-pay</th>\n",
       "      <th>Female</th>\n",
       "      <th>Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15060.000000</td>\n",
       "      <td>1.506000e+04</td>\n",
       "      <td>15060.000000</td>\n",
       "      <td>15060.000000</td>\n",
       "      <td>15060.000000</td>\n",
       "      <td>15060.000000</td>\n",
       "      <td>15060.000000</td>\n",
       "      <td>15060.000000</td>\n",
       "      <td>15060.000000</td>\n",
       "      <td>15060.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>15060.000000</td>\n",
       "      <td>15060.000000</td>\n",
       "      <td>15060.000000</td>\n",
       "      <td>15060.000000</td>\n",
       "      <td>15060.000000</td>\n",
       "      <td>15060.000000</td>\n",
       "      <td>15060.000000</td>\n",
       "      <td>15060.000000</td>\n",
       "      <td>15060.000000</td>\n",
       "      <td>15060.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1120.301594</td>\n",
       "      <td>1.896164e+05</td>\n",
       "      <td>89.041899</td>\n",
       "      <td>38.768327</td>\n",
       "      <td>10.112749</td>\n",
       "      <td>40.951594</td>\n",
       "      <td>0.009894</td>\n",
       "      <td>0.027092</td>\n",
       "      <td>0.093692</td>\n",
       "      <td>0.008101</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213878</td>\n",
       "      <td>0.030744</td>\n",
       "      <td>0.068592</td>\n",
       "      <td>0.731806</td>\n",
       "      <td>0.037981</td>\n",
       "      <td>0.086122</td>\n",
       "      <td>0.044290</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.326228</td>\n",
       "      <td>0.673772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7703.181842</td>\n",
       "      <td>1.056150e+05</td>\n",
       "      <td>406.283245</td>\n",
       "      <td>13.380676</td>\n",
       "      <td>2.558727</td>\n",
       "      <td>12.062831</td>\n",
       "      <td>0.098977</td>\n",
       "      <td>0.162356</td>\n",
       "      <td>0.291409</td>\n",
       "      <td>0.089643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410055</td>\n",
       "      <td>0.172628</td>\n",
       "      <td>0.252768</td>\n",
       "      <td>0.443034</td>\n",
       "      <td>0.191158</td>\n",
       "      <td>0.280554</td>\n",
       "      <td>0.205744</td>\n",
       "      <td>0.021555</td>\n",
       "      <td>0.468848</td>\n",
       "      <td>0.468848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.349200e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.166550e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.779550e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.385888e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>99999.000000</td>\n",
       "      <td>1.490400e+06</td>\n",
       "      <td>3770.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       capital-gain        fnlwgt  capital-loss           age  education-num  \\\n",
       "count  15060.000000  1.506000e+04  15060.000000  15060.000000   15060.000000   \n",
       "mean    1120.301594  1.896164e+05     89.041899     38.768327      10.112749   \n",
       "std     7703.181842  1.056150e+05    406.283245     13.380676       2.558727   \n",
       "min        0.000000  1.349200e+04      0.000000     17.000000       1.000000   \n",
       "25%        0.000000  1.166550e+05      0.000000     28.000000       9.000000   \n",
       "50%        0.000000  1.779550e+05      0.000000     37.000000      10.000000   \n",
       "75%        0.000000  2.385888e+05      0.000000     48.000000      13.000000   \n",
       "max    99999.000000  1.490400e+06   3770.000000     90.000000      16.000000   \n",
       "\n",
       "       hours-per-week  Amer-Indian-Eskimo  Asian-Pac-Islander         Black  \\\n",
       "count    15060.000000        15060.000000        15060.000000  15060.000000   \n",
       "mean        40.951594            0.009894            0.027092      0.093692   \n",
       "std         12.062831            0.098977            0.162356      0.291409   \n",
       "min          1.000000            0.000000            0.000000      0.000000   \n",
       "25%         40.000000            0.000000            0.000000      0.000000   \n",
       "50%         40.000000            0.000000            0.000000      0.000000   \n",
       "75%         45.000000            0.000000            0.000000      0.000000   \n",
       "max         99.000000            1.000000            1.000000      1.000000   \n",
       "\n",
       "              Other  ...  Some-college   Federal-gov     Local-gov  \\\n",
       "count  15060.000000  ...  15060.000000  15060.000000  15060.000000   \n",
       "mean       0.008101  ...      0.213878      0.030744      0.068592   \n",
       "std        0.089643  ...      0.410055      0.172628      0.252768   \n",
       "min        0.000000  ...      0.000000      0.000000      0.000000   \n",
       "25%        0.000000  ...      0.000000      0.000000      0.000000   \n",
       "50%        0.000000  ...      0.000000      0.000000      0.000000   \n",
       "75%        0.000000  ...      0.000000      0.000000      0.000000   \n",
       "max        1.000000  ...      1.000000      1.000000      1.000000   \n",
       "\n",
       "            Private  Self-emp-inc  Self-emp-not-inc     State-gov  \\\n",
       "count  15060.000000  15060.000000      15060.000000  15060.000000   \n",
       "mean       0.731806      0.037981          0.086122      0.044290   \n",
       "std        0.443034      0.191158          0.280554      0.205744   \n",
       "min        0.000000      0.000000          0.000000      0.000000   \n",
       "25%        0.000000      0.000000          0.000000      0.000000   \n",
       "50%        1.000000      0.000000          0.000000      0.000000   \n",
       "75%        1.000000      0.000000          0.000000      0.000000   \n",
       "max        1.000000      1.000000          1.000000      1.000000   \n",
       "\n",
       "        Without-pay        Female          Male  \n",
       "count  15060.000000  15060.000000  15060.000000  \n",
       "mean       0.000465      0.326228      0.673772  \n",
       "std        0.021555      0.468848      0.468848  \n",
       "min        0.000000      0.000000      0.000000  \n",
       "25%        0.000000      0.000000      0.000000  \n",
       "50%        0.000000      0.000000      1.000000  \n",
       "75%        0.000000      1.000000      1.000000  \n",
       "max        1.000000      1.000000      1.000000  \n",
       "\n",
       "[8 rows x 102 columns]"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y = filter_test_data['result'].values\n",
    "test_x = filter_test_data.drop('result',  axis=1).values\n",
    "filter_test_data.drop('result',  axis=1).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Above picture shows test_x and text_y information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(data, types):\n",
    "    ret = data.copy()\n",
    "    ret = ret.replace('?', np.nan).dropna()\n",
    "    if types == 'train':\n",
    "        ret = ret.replace({'result':{'>50K':1, '<=50K': 0}})\n",
    "    else:\n",
    "        ret = ret.replace({'result':{'>50K.':1, '<=50K.': 0}})\n",
    "    ret = ret.reset_index()\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.2 (20%) Derive the gradient and hession matrix for the new E(w)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient \n",
    "$$\n",
    "(\\sum_{n=1}^N (y_n - t_n) * x_n)  +  \\lambda * w\n",
    "$$\n",
    "\n",
    "- Hession \n",
    "$$\n",
    "(\\sum_{n=1}^N y_n *( 1 - y_n) * x_n * x_n^T)  +  \\lambda \n",
    "$$\n",
    "\n",
    "$$\n",
    "y_n = \\sigma_( W^T *x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.3 (30%) Create your own mylogistic_l2 class. Show the learned w as well as test accuracy for the cases below. If w is too long for you, show selected w for continuous-valued, binary-valued, and the constant term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class mylogistic_l2():\n",
    "    \n",
    "    def __init__(self, reg_vec = None, max_iter = 1000, tol = 1e-5, add_intercept = True):\n",
    "        self.reg_vec = reg_vec\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.add_intercept = add_intercept\n",
    "        self.train_x = None\n",
    "        self.train_y = None\n",
    "        self.w_coef = None\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "#         z = np.array([*map(self.replace_extreme, z)])\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    def log_likelihood(self, x, y, w):  \n",
    "        z = np.matmul(x, w).astype(\"float_\")  \n",
    "        sigmoid_probs = self.sigmoid(z)\n",
    "        return -0.5 * np.matmul(np.matmul(w.T, self.reg_vec), w) + np.sum(np.dot(y, np.log(sigmoid_probs))+ np.dot((1 - y) , np.log(1 - sigmoid_probs)))\n",
    "    \n",
    "    def replace_extreme(self, val):\n",
    "        eps = 0.000000001\n",
    "        if val < eps:\n",
    "            val = eps\n",
    "        elif val > 1 - eps:\n",
    "            val = 1 - eps\n",
    "        return val\n",
    "    \n",
    "    def gradient(self, x, y, w,):    \n",
    "        z = np.matmul(x, w).astype(\"float_\")  \n",
    "        sigmoid_probs = self.sigmoid(z)\n",
    "        y = y.reshape(len(y), 1)\n",
    "        # compute gradient\n",
    "#         print(np.shape(np.matmul(np.transpose(x), sigmoid_probs - y)), np.shape(np.matmul(self.reg_vec, w)))\n",
    "#         print(np.shape(np.matmul(np.transpose(x), sigmoid_probs - y) + np.matmul(self.reg_vec, w)))\n",
    "        err_gra = np.matmul(np.transpose(x), sigmoid_probs - y)\n",
    "        return err_gra + np.matmul(self.reg_vec, w)\n",
    "    \n",
    "    def hessian(self, x, w):\n",
    "        z = np.matmul(x, w).astype(\"float_\")  \n",
    "        sigmoid_probs = self.sigmoid(z) \n",
    "        one_sub_sigmoid_probs = 1 - sigmoid_probs\n",
    "        R =  (sigmoid_probs * one_sub_sigmoid_probs).flatten()\n",
    "#         R_nn = np.diag(R)\n",
    "        res = np.multiply(x.T, R)\n",
    "        hes = np.matmul(res, x)\n",
    "#         print(time.time())\n",
    "#         hes = np.linalg.multi_dot( [ x.T, R_nn, x])\n",
    "#         print(time.time())\n",
    "        return (hes + self.reg_vec)\n",
    "        \n",
    "    def newtons_method(self, x, y): \n",
    "        # Initialize log_likelihood & parameters\n",
    "        b = np.diag(self.reg_vec).mean()\n",
    "        b = np.diag(b * np.array([1 for i in range(len(self.reg_vec))]))\n",
    "        \n",
    "        # The intercept term \n",
    "        w = np.linalg.multi_dot( [np.linalg.inv( np.matmul(x.T, x) + b ), x.T , y])\n",
    "        w = w.reshape((len(w), 1))\n",
    "        delta_l = np.Infinity                                          \n",
    "        # Convergence Conditions                                                        \n",
    "        tol = self.tol                                                               \n",
    "        max_iter = self.max_iter                                                           \n",
    "        i = 0\n",
    "        l = self.log_likelihood(x, y, w)\n",
    "         # w -> M x 1 matrix\n",
    "\n",
    "        while abs(delta_l) > tol and i < max_iter :                                       \n",
    "            i += 1                           \n",
    "#             print('iteration : ', i)\n",
    "            g = self.gradient(x, y, w)\n",
    "            hess = self.hessian(x, w)\n",
    "#             print(hess)\n",
    "            H_inv = np.linalg.inv(hess)\n",
    "            g_hess = np.matmul(H_inv, g)\n",
    "            # Perform our update step\n",
    "            w = w - g_hess                                                                \n",
    "            # Update the log-likelihood at each iteration                                     \n",
    "            l_new = self.log_likelihood(x, y, w)  \n",
    "            delta_l = l - l_new \n",
    "            l = l_new\n",
    "        return w                 \n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.train_x = x\n",
    "        self.train_y = y\n",
    "        if self.add_intercept:\n",
    "            intercept = np.zeros((len(self.train_x), 1)) + 1\n",
    "            self.train_x = np.array(np.append(self.train_x, intercept, axis=1))\n",
    "        w_coef = self.newtons_method(self.train_x, self.train_y)\n",
    "#         print('W coefficient is :\\n',w_coef)\n",
    "        self.w_coef = w_coef\n",
    "        \n",
    "    def predict(self, x):\n",
    "        if self.sigmoid(np.matmul(x, self.w_coef))> 0.5 :\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: lambda = 1 for all coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "iteration :  1\n",
      "iteration :  2\n",
      "iteration :  3\n",
      "iteration :  4\n",
      "iteration :  5\n",
      "iteration :  6\n",
      "iteration :  7\n",
      "W coefficient is :\n",
      " [[ 3.16590245e-04]\n",
      " [ 7.26281835e-07]\n",
      " [ 6.38679434e-04]\n",
      " [ 2.48591297e-02]\n",
      " [ 1.85806163e-01]\n",
      " [ 2.90215841e-02]\n",
      " [-1.02439618e+00]\n",
      " [-2.92668732e-01]\n",
      " [-6.35195701e-01]\n",
      " [-8.96890177e-01]\n",
      " [-4.84594618e-01]\n",
      " [ 9.03420736e-01]\n",
      " [ 4.04668560e-01]\n",
      " [-5.46920552e-01]\n",
      " [-1.31374667e+00]\n",
      " [ 4.27865170e-01]\n",
      " [-9.47705967e-01]\n",
      " [-1.07395557e-01]\n",
      " [-4.09492097e-01]\n",
      " [ 3.77034140e-01]\n",
      " [ 5.45751678e-01]\n",
      " [ 5.23589384e-01]\n",
      " [-6.70323916e-01]\n",
      " [-1.09516216e-01]\n",
      " [ 4.53277949e-02]\n",
      " [-1.63971889e-01]\n",
      " [-4.71983494e-02]\n",
      " [-4.02281380e-03]\n",
      " [-3.44708811e-01]\n",
      " [ 1.03362979e-01]\n",
      " [ 4.50497096e-01]\n",
      " [ 8.22875669e-01]\n",
      " [ 9.41389495e-02]\n",
      " [ 2.92357805e-01]\n",
      " [-3.50891475e-01]\n",
      " [-4.39859204e-01]\n",
      " [-4.07586321e-01]\n",
      " [-6.90804284e-01]\n",
      " [-4.47783816e-01]\n",
      " [ 3.91129903e-01]\n",
      " [ 8.03243762e-02]\n",
      " [ 7.97357705e-02]\n",
      " [-1.81402777e-01]\n",
      " [-9.03628229e-02]\n",
      " [-9.81576059e-01]\n",
      " [-7.79910247e-02]\n",
      " [-3.31100773e-01]\n",
      " [-1.88816101e-01]\n",
      " [ 2.89337136e-01]\n",
      " [-8.34197419e-01]\n",
      " [ 5.39068128e-01]\n",
      " [-8.78492150e-02]\n",
      " [-2.46913576e-02]\n",
      " [ 7.15174653e-01]\n",
      " [-1.06504054e+00]\n",
      " [-7.73348089e-01]\n",
      " [-3.53722827e-01]\n",
      " [-9.03671285e-01]\n",
      " [-1.69395623e+00]\n",
      " [ 4.29670534e-01]\n",
      " [ 4.99488476e-01]\n",
      " [ 2.05962313e-01]\n",
      " [ 5.68478621e-01]\n",
      " [-1.78947443e-01]\n",
      " [-1.00975740e+00]\n",
      " [ 1.19612258e+00]\n",
      " [ 8.38966933e-01]\n",
      " [-9.56673066e-01]\n",
      " [-1.50523218e+00]\n",
      " [-1.08232391e+00]\n",
      " [-8.14848363e-01]\n",
      " [-5.76254828e-01]\n",
      " [-3.75781253e-01]\n",
      " [-1.12608337e+00]\n",
      " [-1.50192087e+00]\n",
      " [-5.01524915e-01]\n",
      " [ 7.47819835e-01]\n",
      " [-4.52249247e-01]\n",
      " [-5.44907403e-01]\n",
      " [-3.82721832e-01]\n",
      " [-9.99810289e-02]\n",
      " [-2.62922225e-01]\n",
      " [-6.29791388e-01]\n",
      " [-4.96565839e-01]\n",
      " [-3.08326937e-01]\n",
      " [-1.27584811e-01]\n",
      " [ 1.28534378e-01]\n",
      " [ 5.87953355e-01]\n",
      " [-2.50481306e-01]\n",
      " [ 2.99991415e-01]\n",
      " [-1.37579993e+00]\n",
      " [ 6.82097029e-01]\n",
      " [-1.00989638e-01]\n",
      " [ 1.89694411e-01]\n",
      " [-4.97719501e-01]\n",
      " [-3.10975202e-01]\n",
      " [-1.29181491e-01]\n",
      " [-7.94111556e-01]\n",
      " [-6.18745003e-01]\n",
      " [-1.17270706e+00]\n",
      " [-2.09367501e+00]\n",
      " [-1.24007039e+00]\n",
      " [-3.33374540e+00]]\n"
     ]
    }
   ],
   "source": [
    "lambda_vec = np.identity(train_x.shape[1]+1)\n",
    "print(lambda_vec)\n",
    "# print(lambda_vec)\n",
    "logic1 = mylogistic_l2(lambda_vec, 1000, 1e-5, True)\n",
    "logic1.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is : 0.8480743691899071\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "if logic1.add_intercept:\n",
    "    intercept = np.array([[1] for i in range(len(test_x))])\n",
    "    test_x_inter = np.append(test_x, intercept, axis=1)\n",
    "for i in range(len(test_x)):\n",
    "    res = logic1.predict(test_x_inter[i])\n",
    "    if res == test_y[i]:\n",
    "        count += 1\n",
    "print('The accuracy is :',count/len(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 2.]\n",
      " [3. 4. 5.]\n",
      " [6. 7. 8.]] [0. 1. 2.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  4.],\n",
       "       [ 0.,  4., 10.],\n",
       "       [ 0.,  7., 16.]])"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.arange(9.0).reshape((3, 3))\n",
    "x2 = np.arange(3.0)\n",
    "print(x1,x2)\n",
    "np.multiply(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: lambda = 1 for all but the intercept, no regularization for incercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\n",
      "[[0.99029126 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.99029126 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.99029126 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.99029126 0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.99029126 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.99029126]]\n",
      "iteration :  1\n",
      "iteration :  2\n",
      "iteration :  3\n",
      "iteration :  4\n",
      "iteration :  5\n",
      "iteration :  6\n",
      "iteration :  7\n",
      "W coefficient is :\n",
      " [[ 3.17024478e-04]\n",
      " [ 7.50706804e-07]\n",
      " [ 6.39652126e-04]\n",
      " [ 2.54336822e-02]\n",
      " [ 2.95324923e-01]\n",
      " [ 2.94914512e-02]\n",
      " [-3.72048376e-01]\n",
      " [ 3.94360616e-01]\n",
      " [ 4.30693947e-02]\n",
      " [-2.61235809e-01]\n",
      " [ 1.95854174e-01]\n",
      " [ 1.00965472e+00]\n",
      " [ 5.02337197e-01]\n",
      " [-4.58396920e-01]\n",
      " [-1.24053337e+00]\n",
      " [ 5.27833699e-01]\n",
      " [-8.67620758e-01]\n",
      " [-2.75702542e-02]\n",
      " [-3.15345727e-01]\n",
      " [ 4.72832596e-01]\n",
      " [ 6.28755385e-01]\n",
      " [ 6.23911101e-01]\n",
      " [-5.88433187e-01]\n",
      " [-2.96923356e-02]\n",
      " [ 1.24844059e-01]\n",
      " [-1.43574429e-01]\n",
      " [ 2.48070503e-02]\n",
      " [ 6.14826821e-02]\n",
      " [-2.48769642e-01]\n",
      " [ 1.94794455e-01]\n",
      " [ 5.25952046e-01]\n",
      " [ 9.31972681e-01]\n",
      " [ 1.87606692e-01]\n",
      " [ 3.79606530e-01]\n",
      " [-2.87259337e-01]\n",
      " [-3.10421302e-01]\n",
      " [-3.32413818e-01]\n",
      " [-6.50777756e-01]\n",
      " [-3.81261235e-01]\n",
      " [ 4.89030076e-01]\n",
      " [ 1.76281854e-01]\n",
      " [ 1.74545517e-01]\n",
      " [-7.36414561e-02]\n",
      " [-3.10934765e-02]\n",
      " [-8.98503738e-01]\n",
      " [ 6.72286166e-03]\n",
      " [-2.72168680e-01]\n",
      " [-1.23870297e-01]\n",
      " [ 3.96789072e-01]\n",
      " [-7.54273374e-01]\n",
      " [ 6.10526925e-01]\n",
      " [ 1.64109182e-01]\n",
      " [ 2.28422252e-01]\n",
      " [ 9.64856017e-01]\n",
      " [-8.17967488e-01]\n",
      " [-5.20782272e-01]\n",
      " [-9.91243553e-02]\n",
      " [-6.49283758e-01]\n",
      " [-1.55300394e+00]\n",
      " [ 6.78427763e-01]\n",
      " [ 7.51030496e-01]\n",
      " [ 4.55411798e-01]\n",
      " [ 8.18714334e-01]\n",
      " [ 7.31938650e-02]\n",
      " [-5.26538170e-01]\n",
      " [ 1.61452756e+00]\n",
      " [ 1.36750998e+00]\n",
      " [-4.92456773e-01]\n",
      " [-1.01532721e+00]\n",
      " [-6.05766406e-01]\n",
      " [-3.41948994e-01]\n",
      " [-4.21151177e-02]\n",
      " [ 1.99456835e-01]\n",
      " [-5.83542676e-01]\n",
      " [-9.36996676e-01]\n",
      " [ 7.53014460e-02]\n",
      " [ 1.28789619e+00]\n",
      " [ 9.08118759e-02]\n",
      " [-1.06210390e-01]\n",
      " [-5.68864876e-02]\n",
      " [ 7.06129017e-01]\n",
      " [ 5.36014978e-01]\n",
      " [ 1.16361326e-01]\n",
      " [ 1.37527872e-01]\n",
      " [-4.01951495e-01]\n",
      " [-1.13370201e-01]\n",
      " [-7.43801938e-02]\n",
      " [ 6.79669927e-02]\n",
      " [-1.95742295e-02]\n",
      " [-1.15929725e-02]\n",
      " [-1.15969699e+00]\n",
      " [ 2.67049827e-01]\n",
      " [ 2.18010722e-02]\n",
      " [ 7.05717318e-01]\n",
      " [ 1.78026502e-02]\n",
      " [ 2.09126595e-01]\n",
      " [ 3.82731145e-01]\n",
      " [-2.79821710e-01]\n",
      " [-1.04552949e-01]\n",
      " [-9.31003049e-01]\n",
      " [-4.27150728e-01]\n",
      " [ 4.27150728e-01]\n",
      " [-8.88722778e+00]]\n"
     ]
    }
   ],
   "source": [
    "lambda_vec = np.identity(train_x.shape[1]+1)\n",
    "lambda_vec [-1, -1] = 0\n",
    "logic2 = mylogistic_l2(lambda_vec, 1000, 1e-5, True)\n",
    "logic2.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is : 0.847808764940239\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "if logic2.add_intercept:\n",
    "    intercept = np.array([[1] for i in range(len(test_x))])\n",
    "    test_x_inter = np.append(test_x, intercept, axis=1)\n",
    "for i in range(len(test_x_inter)):\n",
    "    res = logic2.predict(test_x_inter[i])\n",
    "    if res == test_y[i]:\n",
    "        count += 1\n",
    "print('The accuracy is :',count/len(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3: lambda = 1 for numerical-valued features, lambda = 0.5 for binary-valued features, no regularization for incercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  0.  0.  ... 0.  0.  0. ]\n",
      " [0.  1.  0.  ... 0.  0.  0. ]\n",
      " [0.  0.  1.  ... 0.  0.  0. ]\n",
      " ...\n",
      " [0.  0.  0.  ... 0.5 0.  0. ]\n",
      " [0.  0.  0.  ... 0.  0.5 0. ]\n",
      " [0.  0.  0.  ... 0.  0.  0. ]]\n",
      "iteration :  1\n",
      "iteration :  2\n",
      "iteration :  3\n",
      "iteration :  4\n",
      "iteration :  5\n",
      "iteration :  6\n",
      "iteration :  7\n",
      "W coefficient is :\n",
      " [[ 3.17319918e-04]\n",
      " [ 7.51944447e-07]\n",
      " [ 6.40115467e-04]\n",
      " [ 2.54757380e-02]\n",
      " [ 3.19092334e-01]\n",
      " [ 2.95136416e-02]\n",
      " [-3.83664223e-01]\n",
      " [ 4.13058389e-01]\n",
      " [ 4.13741722e-02]\n",
      " [-2.63868110e-01]\n",
      " [ 1.93099772e-01]\n",
      " [ 1.18913504e+00]\n",
      " [ 5.50823621e-01]\n",
      " [-4.76703416e-01]\n",
      " [-1.45906507e+00]\n",
      " [ 5.82266388e-01]\n",
      " [-1.06198169e+00]\n",
      " [-9.41595511e-03]\n",
      " [-3.18451269e-01]\n",
      " [ 5.24213619e-01]\n",
      " [ 7.29262486e-01]\n",
      " [ 6.74417704e-01]\n",
      " [-6.38234075e-01]\n",
      " [-9.70370974e-03]\n",
      " [ 1.74243036e-01]\n",
      " [-2.36216830e-01]\n",
      " [ 3.80747425e-02]\n",
      " [ 1.00364860e-01]\n",
      " [-2.47188656e-01]\n",
      " [ 2.38206872e-01]\n",
      " [ 6.41987645e-01]\n",
      " [ 1.00609325e+00]\n",
      " [ 2.33159600e-01]\n",
      " [ 4.22777999e-01]\n",
      " [-3.53092034e-01]\n",
      " [-2.90744304e-01]\n",
      " [-3.80715988e-01]\n",
      " [-9.62423309e-01]\n",
      " [-4.49691900e-01]\n",
      " [ 5.13214010e-01]\n",
      " [ 2.19833007e-01]\n",
      " [ 2.27002306e-01]\n",
      " [-5.01168975e-02]\n",
      " [-1.79083299e-02]\n",
      " [-9.59642325e-01]\n",
      " [ 1.67396852e-02]\n",
      " [-3.27258263e-01]\n",
      " [-1.39435434e-01]\n",
      " [ 4.28358927e-01]\n",
      " [-8.46070010e-01]\n",
      " [ 7.51083104e-01]\n",
      " [ 2.36234568e-01]\n",
      " [ 3.00253587e-01]\n",
      " [ 1.03825742e+00]\n",
      " [-7.52670195e-01]\n",
      " [-4.53424296e-01]\n",
      " [-2.69070976e-02]\n",
      " [-5.82347486e-01]\n",
      " [-2.00233663e+00]\n",
      " [ 7.51037287e-01]\n",
      " [ 8.27361959e-01]\n",
      " [ 5.28329988e-01]\n",
      " [ 8.95058089e-01]\n",
      " [ 1.45241736e-01]\n",
      " [-5.72016479e-01]\n",
      " [ 1.82571076e+00]\n",
      " [ 1.39641686e+00]\n",
      " [-5.47067030e-01]\n",
      " [-1.05936555e+00]\n",
      " [-6.55642109e-01]\n",
      " [-3.88036449e-01]\n",
      " [-8.34438876e-02]\n",
      " [ 2.32698984e-01]\n",
      " [-5.92702310e-01]\n",
      " [-9.22764380e-01]\n",
      " [ 1.11240494e-01]\n",
      " [ 1.25497110e+00]\n",
      " [ 2.18992444e-01]\n",
      " [-2.26135094e-03]\n",
      " [ 2.18816221e-02]\n",
      " [ 9.74981515e-01]\n",
      " [ 7.54034090e-01]\n",
      " [ 2.92224599e-01]\n",
      " [ 2.93070047e-01]\n",
      " [-4.19612047e-01]\n",
      " [-1.04346432e-01]\n",
      " [-1.12963353e-01]\n",
      " [-3.78241652e-02]\n",
      " [ 3.76517948e-02]\n",
      " [-7.30515953e-02]\n",
      " [-2.08394381e+00]\n",
      " [ 1.86338326e-01]\n",
      " [ 5.48283203e-02]\n",
      " [ 7.66990129e-01]\n",
      " [ 7.62915572e-02]\n",
      " [ 2.68338062e-01]\n",
      " [ 4.43127805e-01]\n",
      " [-2.20821183e-01]\n",
      " [-4.63421363e-02]\n",
      " [-1.28758423e+00]\n",
      " [-4.29102280e-01]\n",
      " [ 4.29102280e-01]\n",
      " [-9.31135249e+00]]\n"
     ]
    }
   ],
   "source": [
    "lambda_vec = np.identity(train_x.shape[1]+1)\n",
    "for i in range(len(lambda_vec)):\n",
    "    if i >= 5:\n",
    "        lambda_vec[i][i] = 0.5\n",
    "lambda_vec [-1, -1] = 0\n",
    "logic3 = mylogistic_l2(lambda_vec, 1000, 1e-5, True)\n",
    "logic3.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is : 0.847675962815405\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "if logic3.add_intercept:\n",
    "    intercept = np.array([[1] for i in range(len(test_x))])\n",
    "    test_x_inter = np.append(test_x, intercept, axis=1)\n",
    "for i in range(len(test_x_inter)):\n",
    "    res = logic3.predict(test_x_inter[i])\n",
    "    if res == test_y[i]:\n",
    "        count += 1\n",
    "print('The accuracy is :', count/len(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.4 (10%) Further split the training data into subtraining (90%) and tuning (10%) to search for the best hyper-parameters. Set the regularization coefficient for the constant term to zero. Allow different regularizations for continuous-valued and binary-valued features. Let  a1  and  a2  denote the regularization coefficients for continuous-valued and binary-valued features. Search the best  a1  and  a2  and report the test accuracy using the best hyper-parameters. You should follow the following procedure to search for the best hyper-parameters.\n",
    "\n",
    "    Choose a set of grids among a reasonable range. For example, 10 grids in [0.01, 100].\n",
    "    Conduct grid search with the constraint that  a1=a2 . Record the best value  a∗1  and  a∗2 .\n",
    "    Fix  a1=a∗1 , and search  a2  for the best value, call the result the new  a∗2 .\n",
    "    Fix  a2=a∗2 , and search  a1  for the best value.\n",
    "    Report the selected  a1  and  a2 .\n",
    "    Train a model using the selected hyper-parameters, and report the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lambda_vector(continuous, binary, intercept = 0):\n",
    "    lambda_vec = list()\n",
    "    for i in range(train_x.shape[1]+1):\n",
    "        if i < 6:\n",
    "            lambda_vec.append (continuous)\n",
    "        elif i < train_x.shape[1]:\n",
    "            lambda_vec.append(binary)\n",
    "        else:\n",
    "            lambda_vec.append(0)\n",
    "        i += 1\n",
    "    return  np.diag(lambda_vec)\n",
    "\n",
    "def get_accuracy(logis, x, obs):\n",
    "    correct = 0\n",
    "    for i in range(len(obs)):\n",
    "        if logis.predict(x[i]) == obs[i]:\n",
    "            correct += 1\n",
    "    return(correct/len(obs))\n",
    "\n",
    "def find_best_a(subtrain_x, tuning_x, subtrain_y, tuning_y, a_set, a1 = None, a2 = None, ):\n",
    "    best_accu = 0\n",
    "    best_a = 0\n",
    "    flag_1 = 0\n",
    "    flag_2 = 0\n",
    "    if a1 == None:\n",
    "        flag_1 =1\n",
    "    if a2 == None:\n",
    "        flag_2 =1 \n",
    "    for a in a_set:\n",
    "        if flag_1 :\n",
    "            a1 = a\n",
    "        if flag_2:\n",
    "            a2 = a\n",
    "        lambda_vec = get_lambda_vector(a1, a2)\n",
    "        mylogistic = mylogistic_l2(reg_vec = lambda_vec, max_iter = 1000, tol = 0.00001, add_intercept = True)\n",
    "        if mylogistic.add_intercept:\n",
    "            intercept = np.array([[1] for i in range(len(tuning_x))])\n",
    "            tuning_x_inter = np.append(tuning_x, intercept, axis=1)\n",
    "        mylogistic.fit(subtrain_x, subtrain_y)\n",
    "        accuracy = get_accuracy(mylogistic, tuning_x_inter, tuning_y)\n",
    "        if accuracy > best_accu:\n",
    "            best_accu = accuracy\n",
    "            best_a = a\n",
    "        print(a, accuracy)\n",
    "    return  best_a, best_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.8508452104739808\n",
      "0.027825594022071243 0.851508120649652\n",
      "0.0774263682681127 0.8518395757374876\n",
      "0.21544346900318834 0.8518395757374876\n",
      "0.5994842503189409 0.8525024859131588\n",
      "1.6681005372000592 0.8528339410009944\n",
      "4.6415888336127775 0.8528339410009944\n",
      "12.915496650148826 0.8511766655618164\n",
      "35.93813663804626 0.8505137553861452\n",
      "100.0 0.8491879350348028\n",
      "best a1=a2 is 1.6681005372000592 accuracy is 0.8528339410009944\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import time \n",
    "subtrain_x, tuning_x, subtrain_y, tuning_y = train_test_split(train_x, train_y, test_size = 0.1)\n",
    "a_set = np.logspace(-2, 2, 10) \n",
    "# print('a value set:\\n', a_set)\n",
    "a1, accu = find_best_a(subtrain_x, tuning_x, subtrain_y, tuning_y, a_set)\n",
    "print(f'best a1=a2 is {a1}', f'accuracy is {accu}')\n",
    "res_1 = a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.8508452104739808\n",
      "0.027825594022071243 0.851508120649652\n",
      "0.0774263682681127 0.8518395757374876\n",
      "0.21544346900318834 0.8518395757374876\n",
      "0.5994842503189409 0.8525024859131588\n",
      "1.6681005372000592 0.8528339410009944\n",
      "4.6415888336127775 0.8528339410009944\n",
      "12.915496650148826 0.8511766655618164\n",
      "35.93813663804626 0.8505137553861452\n",
      "100.0 0.8491879350348028\n",
      "best when a1 fixed at 1.6681005372000592, a2 is 1.6681005372000592 accuracy is 0.8528339410009944\n"
     ]
    }
   ],
   "source": [
    "a2, accu = find_best_a(subtrain_x, tuning_x, subtrain_y, tuning_y, a_set, a1)\n",
    "print(f'best when a1 fixed at {res_1}, a2 is {a2}', f'accuracy is {accu}')\n",
    "res_2 = a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.8528339410009944\n",
      "0.027825594022071243 0.8528339410009944\n",
      "0.0774263682681127 0.8528339410009944\n",
      "0.21544346900318834 0.8528339410009944\n",
      "0.5994842503189409 0.8528339410009944\n",
      "1.6681005372000592 0.8528339410009944\n",
      "4.6415888336127775 0.8528339410009944\n",
      "12.915496650148826 0.8528339410009944\n",
      "35.93813663804626 0.8528339410009944\n",
      "100.0 0.8528339410009944\n",
      "best when a2 fixed at 1.6681005372000592, a1 is 0.01 accuracy is 0.8528339410009944\n"
     ]
    }
   ],
   "source": [
    "a1, accu = find_best_a(subtrain_x, tuning_x, subtrain_y, tuning_y, a_set, None, a2)\n",
    "print(f'best when a2 fixed at {res_2}, a1 is {a1}', f'accuracy is {accu}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.5 (20%) Use sklearn.linear_model.LogisticRegression to train and test the model (including hyper-parameter tuning). Compare the estimated parameters and test accuracy with those from your own models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'C': array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]), 'penalty': ['l2'], 'solver': ['newton-cg']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l2\"], 'solver':['newton-cg']}\n",
    "clf = LogisticRegression()\n",
    "clf_cv = GridSearchCV(clf, grid, cv=10)\n",
    "clf_cv.fit(np.array(train_x), np.array(train_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters:  {'C': 100.0, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "accuracy : 0.8488495457860885\n"
     ]
    }
   ],
   "source": [
    "print(\"best parameters: \",clf_cv.best_params_)\n",
    "print(\"accuracy :\",clf_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/miniconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.8479415670650731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(C= 100.0, penalty= 'l2', solver = 'newton-cg').fit(train_x, train_y)\n",
    "res = clf.predict(test_x)\n",
    "count = 0\n",
    "for i in range(len(test_x)):\n",
    "    if res[i] == test_y[i]:\n",
    "        count += 1\n",
    "print('accuracy :',count/len(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using sklearn package, I get highest accuracy with 0.8479415670650731"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
